{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e9d2c735-fb11-4566-833d-d60aa77c9d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from google.colab import drive\n",
    "\n",
    "#drive.mount('/content/gdrive')\n",
    "#root = '/content/gdrive/MyDrive/Colab Notebooks/dataset/'\n",
    "\n",
    "root = './Dataset/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7ac78563-70a9-4d80-9e52-8d52a1b9fe16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from tqdm.notebook import tqdm\n",
    "import time\n",
    "from torch.optim import Adam\n",
    "from torch import nn\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import AutoConfig, AutoModel, AutoTokenizer, AutoModelForSequenceClassification\n",
    "from sklearn.utils import shuffle\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "import emoji\n",
    "import copy\n",
    "import pickle\n",
    "pd.set_option('display.max_colwidth', None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "47c14dc0-f30f-4b8d-9971-edd6d28b8d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_hashtag(dataset):\n",
    "    augmented_dataset = dataset.copy()\n",
    "    augmented_dataset['hashtag'] = ' '\n",
    "    data = []\n",
    "\n",
    "    for i in tqdm(range(len(dataset)), desc= \"Looking for hashtag\"):\n",
    "        text = dataset.loc[i,'text']\n",
    "        dato = [k for k in text.split() if k.startswith(\"#\")]\n",
    "        data.append(dato)\n",
    "\n",
    "    augmented_dataset['hashtag'] = data\n",
    "    augmented_dataset['hashtag'] = augmented_dataset['hashtag'].apply(lambda x: \" \".join(x).replace('#', ''))\n",
    "\n",
    "\n",
    "    return augmented_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f5c26eee-35a8-4de4-867b-401b8495f871",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def add_emoji(dataset):\n",
    "\n",
    "    augmented_dataset = dataset.copy()\n",
    "    augmented_dataset['emoji'] = ' '\n",
    "\n",
    "    for i in tqdm(range(len(dataset)), desc= \"Looking for emoji\"):\n",
    "        text = dataset.loc[i,'text']\n",
    "        data = emoji.demojize(text, language='it')\n",
    "        pattern = r\":(\\w+):\"\n",
    "\n",
    "        emoji_found = re.findall(pattern,data)\n",
    "        emoji_found = ' '.join(emoji_found)\n",
    "        augmented_dataset.at[i,'emoji'] = emoji_found\n",
    "\n",
    "    return augmented_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ec17736d-3e0b-4796-9310-e4173665233a",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = {\n",
    "    \"#_classes\" : 1,\n",
    "    \"epochs\": 30,\n",
    "    \"learning_rate\": 1e-6,\n",
    "    \"batch_size\": 3,\n",
    "    \"dropout\": 0.1,\n",
    "    \"stopwords\": False,\n",
    "    \"h_dim\": 768,\n",
    "    \"patience\": 10,\n",
    "    \"min_delta\": 0.01,\n",
    "    \"language_model\": \"bert-base-multilingual-cased\",\n",
    "    \"extra_features\": 65, #32 emoji + 32 hashtag + 1 char_count\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "842be5b3-789d-47fe-b302-0e358c835349",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassifierDeep(nn.Module):\n",
    "\n",
    "    def __init__(self, labels, hdim, dropout, model_name,extra_features = hyperparameters['extra_features']):\n",
    "        super(ClassifierDeep, self).__init__()\n",
    "        config = AutoConfig.from_pretrained(model_name)\n",
    "        self.lm_model = AutoModel.from_pretrained(model_name, config=config)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(hdim + extra_features, 512),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 128),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 1),\n",
    "            nn.Sigmoid()\n",
    "            )\n",
    "\n",
    "    def forward(self, input_id_text, attention_mask, char_count,emoji_text, hashtag):\n",
    "        output = self.lm_model(input_id_text, attention_mask).last_hidden_state\n",
    "        output = output[:,0,:]\n",
    "        output = torch.cat((output, char_count.unsqueeze(-1), emoji_text, hashtag), dim=1)  # Concatena il conteggio dei caratteri\n",
    "        return self.classifier(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7811027d-a4ca-494c-abbc-248cbc91ff5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a7c8e2b7ccf4bcd95345f1273ea311a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Looking for emoji:   0%|          | 0/460 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d9f203182a04d618501a9caf5976061",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Looking for hashtag:   0%|          | 0/460 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "721a1c2f4f4845a6ad659cb552e5054e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/92 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>predicted_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>[1.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>[0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>[0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>[0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>[0.0]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id predicted_label\n",
       "0   0           [1.0]\n",
       "1   1           [0.0]\n",
       "2   2           [0.0]\n",
       "3   3           [0.0]\n",
       "4   4           [0.0]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"best_weight_original.pkl\", 'rb') as f:\n",
    "  weights = pickle.load(f)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(hyperparameters[\"language_model\"])\n",
    "\n",
    "\n",
    "result_dataset = pd.DataFrame(columns=['id', 'predicted_label'])\n",
    "\n",
    "\n",
    "\n",
    "model = ClassifierDeep(hyperparameters[\"#_classes\"],\n",
    "                    hyperparameters[\"h_dim\"],\n",
    "                    hyperparameters[\"dropout\"],\n",
    "                    hyperparameters[\"language_model\"]).to(device)\n",
    "model.load_state_dict(weights)\n",
    "\n",
    "model.eval()\n",
    "\n",
    "test_set = pd.read_csv(f'{root}subtaskA_test.csv',  header=0, names=['id', 'text'])\n",
    "test_set['text'] = test_set['text'].apply(lambda x: x.replace('\\r', ' ').replace('\\n', ' '))\n",
    "test_set['char_count'] = test_set['text'].str.len()\n",
    "test_set = add_emoji(test_set)\n",
    "test_set = add_hashtag(test_set)\n",
    "test_set.drop(columns=['id'], inplace=True)\n",
    "\n",
    "batch_size = 5\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in tqdm(range(0,len(test_set),batch_size)):\n",
    "        tokens = tokenizer(list(test_set[\"text\"].iloc[i:i+batch_size]), add_special_tokens=True,\n",
    "                                   return_tensors='pt', padding='max_length',\n",
    "                                   max_length = 512, truncation=True)\n",
    "    \n",
    "        tokens_emoji = tokenizer(list(test_set[\"emoji\"].iloc[i:i+batch_size]), add_special_tokens=True,\n",
    "                            return_tensors='pt', padding='max_length',\n",
    "                            max_length = 32, truncation=True)\n",
    "        tokens_hashtag = tokenizer(list(test_set[\"hashtag\"].iloc[i:i+batch_size]), add_special_tokens=True,\n",
    "                                return_tensors='pt', padding='max_length',\n",
    "                                max_length = 32, truncation=True)\n",
    "    \n",
    "        input_id_texts = tokens['input_ids'].squeeze(1).to(device)\n",
    "        mask_texts = tokens['attention_mask'].squeeze(1).to(device)\n",
    "    \n",
    "        input_id_emoji = tokens_emoji['input_ids'].squeeze(1).to(device)\n",
    "        mask_emoji = tokens_emoji['attention_mask'].squeeze(1).to(device)\n",
    "    \n",
    "        input_id_hashtag = tokens_hashtag['input_ids'].squeeze(1).to(device)\n",
    "        mask_hashtag = tokens_hashtag['attention_mask'].squeeze(1).to(device)\n",
    "    \n",
    "        batch_char_count = [torch.tensor(char_count) for char_count in test_set[\"char_count\"].iloc[i:i+batch_size]] \n",
    "        batch_char_count = torch.stack(batch_char_count)\n",
    "        batch_char_count=batch_char_count.to(device)\n",
    "    \n",
    "        output = model(input_id_texts, mask_texts,batch_char_count,input_id_emoji, input_id_hashtag)\n",
    "        \n",
    "        input_id_texts = input_id_texts.detach().cpu()\n",
    "        mask_texts = mask_texts.detach().cpu()\n",
    "        output = output.detach().cpu()\n",
    "        batch_char_count = batch_char_count.detach().cpu()\n",
    "        output = output.round().numpy().tolist()\n",
    "        current_results = pd.DataFrame({'predicted_label':output})\n",
    "        result_dataset = pd.concat([result_dataset,current_results],ignore_index=True)\n",
    "\n",
    "\n",
    "result_dataset['id'] = result_dataset.index\n",
    "\n",
    "\n",
    "result_dataset.to_csv('results_original_datset.csv')\n",
    "result_dataset.head()\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0e4169ec-87a2-4db6-aeb0-6e46945ac375",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2aeabd8778a461194363541ba4fb25e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Looking for emoji:   0%|          | 0/460 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b55df7b277f4115b736d5264bcbc186",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Looking for hashtag:   0%|          | 0/460 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5dec55bd973d4ed0a51678477e539070",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/92 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>predicted_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>[1.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>[0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>[0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>[0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>[0.0]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id predicted_label\n",
       "0   0           [1.0]\n",
       "1   1           [0.0]\n",
       "2   2           [0.0]\n",
       "3   3           [0.0]\n",
       "4   4           [0.0]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"best_weight.pkl\", 'rb') as f:\n",
    "  weights = pickle.load(f)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(hyperparameters[\"language_model\"])\n",
    "\n",
    "\n",
    "result_dataset = pd.DataFrame(columns=['id', 'predicted_label'])\n",
    "\n",
    "\n",
    "\n",
    "model = ClassifierDeep(hyperparameters[\"#_classes\"],\n",
    "                    hyperparameters[\"h_dim\"],\n",
    "                    hyperparameters[\"dropout\"],\n",
    "                    hyperparameters[\"language_model\"]).to(device)\n",
    "model.load_state_dict(weights)\n",
    "\n",
    "model.eval()\n",
    "\n",
    "test_set = pd.read_csv(f'{root}subtaskA_test.csv',  header=0, names=['id', 'text'])\n",
    "test_set['text'] = test_set['text'].apply(lambda x: x.replace('\\r', ' ').replace('\\n', ' '))\n",
    "test_set['char_count'] = test_set['text'].str.len()\n",
    "test_set = add_emoji(test_set)\n",
    "test_set = add_hashtag(test_set)\n",
    "test_set.drop(columns=['id'], inplace=True)\n",
    "\n",
    "batch_size = 5\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in tqdm(range(0,len(test_set),batch_size)):\n",
    "        tokens = tokenizer(list(test_set[\"text\"].iloc[i:i+batch_size]), add_special_tokens=True,\n",
    "                                   return_tensors='pt', padding='max_length',\n",
    "                                   max_length = 512, truncation=True)\n",
    "    \n",
    "        tokens_emoji = tokenizer(list(test_set[\"emoji\"].iloc[i:i+batch_size]), add_special_tokens=True,\n",
    "                            return_tensors='pt', padding='max_length',\n",
    "                            max_length = 32, truncation=True)\n",
    "        tokens_hashtag = tokenizer(list(test_set[\"hashtag\"].iloc[i:i+batch_size]), add_special_tokens=True,\n",
    "                                return_tensors='pt', padding='max_length',\n",
    "                                max_length = 32, truncation=True)\n",
    "    \n",
    "        input_id_texts = tokens['input_ids'].squeeze(1).to(device)\n",
    "        mask_texts = tokens['attention_mask'].squeeze(1).to(device)\n",
    "    \n",
    "        input_id_emoji = tokens_emoji['input_ids'].squeeze(1).to(device)\n",
    "        mask_emoji = tokens_emoji['attention_mask'].squeeze(1).to(device)\n",
    "    \n",
    "        input_id_hashtag = tokens_hashtag['input_ids'].squeeze(1).to(device)\n",
    "        mask_hashtag = tokens_hashtag['attention_mask'].squeeze(1).to(device)\n",
    "    \n",
    "        batch_char_count = [torch.tensor(char_count) for char_count in test_set[\"char_count\"].iloc[i:i+batch_size]] \n",
    "        batch_char_count = torch.stack(batch_char_count)\n",
    "        batch_char_count=batch_char_count.to(device)\n",
    "    \n",
    "        output = model(input_id_texts, mask_texts,batch_char_count,input_id_emoji, input_id_hashtag)\n",
    "        \n",
    "        input_id_texts = input_id_texts.detach().cpu()\n",
    "        mask_texts = mask_texts.detach().cpu()\n",
    "        output = output.detach().cpu()\n",
    "        batch_char_count = batch_char_count.detach().cpu()\n",
    "        output = output.round().numpy().tolist()\n",
    "        current_results = pd.DataFrame({'predicted_label':output})\n",
    "        result_dataset = pd.concat([result_dataset,current_results],ignore_index=True)\n",
    "\n",
    "\n",
    "result_dataset['id'] = result_dataset.index\n",
    "\n",
    "\n",
    "result_dataset.to_csv('results_augmented_datset.csv')\n",
    "result_dataset.head()\n",
    "\n",
    "            "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
